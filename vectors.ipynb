{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bca7cb01-21c2-4ca0-8c1b-8f4a43ed659a",
   "metadata": {
    "executionInfo": {
     "elapsed": 836,
     "status": "ok",
     "timestamp": 1644256260747,
     "user": {
      "displayName": "Ori Suchy",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02082319045580391969"
     },
     "user_tz": -120
    },
    "id": "bca7cb01-21c2-4ca0-8c1b-8f4a43ed659a"
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import lxml\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import random\n",
    "random.seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "gmfwIocXo8ln",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25698,
     "status": "ok",
     "timestamp": 1644256286685,
     "user": {
      "displayName": "Ori Suchy",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02082319045580391969"
     },
     "user_tz": -120
    },
    "id": "gmfwIocXo8ln",
    "outputId": "92235f8a-3af3-4e64-d760-97c257e73db1"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "V0_LNe2jpqFv",
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1644256286685,
     "user": {
      "displayName": "Ori Suchy",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02082319045580391969"
     },
     "user_tz": -120
    },
    "id": "V0_LNe2jpqFv"
   },
   "outputs": [],
   "source": [
    "# folder_path = '/content/drive/MyDrive/לימודים/NLP mini project/'\n",
    "folder_path = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645fe6bf-f2e6-4a5a-91c2-7ecc9691f4d0",
   "metadata": {
    "id": "645fe6bf-f2e6-4a5a-91c2-7ecc9691f4d0"
   },
   "source": [
    "---\n",
    "# Preprocessing data:\n",
    "#### From TanakhDictaTEI we iterate over each book and retreive only sentences (\"psukim\") that contain the word \"ACH\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65ac9be8-0601-423b-b762-4f219d691683",
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1644256286686,
     "user": {
      "displayName": "Ori Suchy",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02082319045580391969"
     },
     "user_tz": -120
    },
    "id": "65ac9be8-0601-423b-b762-4f219d691683"
   },
   "outputs": [],
   "source": [
    "path_torah =os.path.join(folder_path, 'TanakhDictaTEI-1', 'Torah')\n",
    "path_prohets = os.path.join(folder_path, 'TanakhDictaTEI-1', 'Prophets')\n",
    "path_writings = os.path.join(folder_path, 'TanakhDictaTEI-1', 'Writings')\n",
    "paths = [path_torah, path_prohets, path_writings]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43ca54f-0581-4908-8057-1f8d111b2289",
   "metadata": {
    "id": "f43ca54f-0581-4908-8057-1f8d111b2289"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b>  TEI from \"moodle\" should be stored in the same directory as the notebook\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46204bbf-3eda-4997-bb5a-b23a0a509c5e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 72135,
     "status": "ok",
     "timestamp": 1644256358813,
     "user": {
      "displayName": "Ori Suchy",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02082319045580391969"
     },
     "user_tz": -120
    },
    "id": "46204bbf-3eda-4997-bb5a-b23a0a509c5e",
    "outputId": "9c3289d3-f012-4a23-9fdb-d10202ae1cbc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 3/3 [00:54<00:00, 18.23s/it]\n"
     ]
    }
   ],
   "source": [
    "achs = []\n",
    "i=2\n",
    "for path in tqdm(paths):\n",
    "    # print(path)\n",
    "    dir = os.fsencode(path)\n",
    "    for file in sorted(os.listdir(dir)):\n",
    "        filename = os.fsdecode(file)\n",
    "        # print(filename)\n",
    "        with open(os.path.join(path, filename), 'r') as f:\n",
    "            content = f.readlines()\n",
    "            content = \"\".join(content)\n",
    "            bs_content = bs(content, \"lxml\")\n",
    "            senteces = bs_content.find_all('s')\n",
    "            for s in senteces:\n",
    "                words = s.find_all('w')\n",
    "                for w in words:\n",
    "                    dtoken = w.get_attribute_list('dtoken')\n",
    "                    if dtoken[0] is not None:\n",
    "                        if \"אך_אַךְ\" in dtoken[0]:\n",
    "                            # print(i)\n",
    "                            # i += 1\n",
    "                            achs.append(s)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66ae391-4f2e-4b4d-aedc-5e00b91d8109",
   "metadata": {},
   "outputs": [],
   "source": [
    "ach_tags = pd.read_csv(os.path.join(folder_path,'Ach.csv'))['תיוג']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "73848c66-e936-4746-845b-3363d81e5c30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d2acb674-0a75-4160-9889-816b80b5fe57",
   "metadata": {
    "id": "d2acb674-0a75-4160-9889-816b80b5fe57"
   },
   "source": [
    "---\n",
    "# Retreiving neighbors in different window sizes:\n",
    "#### For each word in sentence we wish to get the \"window\" that surrounds it. \n",
    "#### We iterate over the tags <w> in each sentence \\<s\\> and append the tags: <br>\n",
    "> For window size 2:\n",
    "    (<t\\-1> , <t+1>) <br>\n",
    "    For window size 4:\n",
    "    (<t\\-2>, <t\\-1>, <t+1>, <t+2>) <br>\n",
    "\n",
    "#### These lists contain the data needed for the feature vectors for our learning models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "1c096394-6498-4322-bc6f-4c6fd791a30e",
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1644256359293,
     "user": {
      "displayName": "Ori Suchy",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02082319045580391969"
     },
     "user_tz": -120
    },
    "id": "1c096394-6498-4322-bc6f-4c6fd791a30e"
   },
   "outputs": [],
   "source": [
    "def w_vec(achs=achs):\n",
    "    tmp = 0\n",
    "    vec2, vec4 = [], []\n",
    "    for s in achs:\n",
    "        words = s.find_all('w')\n",
    "        i = 0\n",
    "        #get index of ACH\n",
    "        for w in words:\n",
    "            dtoken = w.get_attribute_list('dtoken')\n",
    "            if \"אך_אַךְ\" not in dtoken[0]:\n",
    "                i += 1\n",
    "            elif s==tmp:\n",
    "                # TODO fix twice appearance:\n",
    "                # print(s.get_attribute_list('displayname_eng')[0])\n",
    "                # print(i)\n",
    "                continue\n",
    "            else:\n",
    "                tmp = s\n",
    "                break\n",
    "        if i < 1:\n",
    "            vec_2 = [-1, words[1]]\n",
    "            vec_4 = [-1, -1, words[1], words[2]]\n",
    "        elif i < 2:\n",
    "            vec_2 = [words[i-1], words[i+1]]\n",
    "            vec_4 = [-1, words[i-1], words[i+1], words[i+2]]\n",
    "        else:\n",
    "            # if (s.get_attribute_list('displayname_eng')[0]) == 'Pasuk 30': print(words)\n",
    "            vec_2 = [words[i-1], words[i+1]]\n",
    "            if len(words) < i+3:\n",
    "                vec_4 = [words[i-2], words[i-1],words[i+1], -1]\n",
    "                \n",
    "            elif len(words) < i+2:\n",
    "                vec_4 = [words[i-2], words[i-1],-1, -1]\n",
    "            else: \n",
    "                vec_4 = [words[i-2], words[i-1],words[i+1], words[i+2]]\n",
    "        vec2.append(vec_2)\n",
    "        vec4.append(vec_4)\n",
    "    return vec2, vec4  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "03961d90-c802-430a-89ca-6c9c9fee4922",
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1644256359294,
     "user": {
      "displayName": "Ori Suchy",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02082319045580391969"
     },
     "user_tz": -120
    },
    "id": "03961d90-c802-430a-89ca-6c9c9fee4922"
   },
   "outputs": [],
   "source": [
    "vec2, vec4 = w_vec() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4132fc91-5bd1-4876-8add-c83c54e2a446",
   "metadata": {
    "id": "4132fc91-5bd1-4876-8add-c83c54e2a446"
   },
   "source": [
    "---\n",
    "\n",
    "\n",
    "# Feature vectors:\n",
    "#### For our classification models, we first need to constract feature representation of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9c0ed1-6288-4e06-a71c-a4ff5c8c9f7e",
   "metadata": {},
   "source": [
    "## Base features:\n",
    "\n",
    "> Vocabulay features <br>\n",
    "Morphological features <br>\n",
    "Syntactic features <br>\n",
    "Lemmatizing features <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "2f9425fc-60fa-4482-b14b-507f19db2027",
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1644256359295,
     "user": {
      "displayName": "Ori Suchy",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02082319045580391969"
     },
     "user_tz": -120
    },
    "id": "2f9425fc-60fa-4482-b14b-507f19db2027"
   },
   "outputs": [],
   "source": [
    "# vocabulay\n",
    "def feature_vec_1(achs=achs, vec2 = vec2, vec4=vec4):\n",
    "    vec_vocab2, vec_vocab4 = [], []\n",
    "    for vec in vec2:\n",
    "        tmp = (list(map((lambda x: x.get_attribute_list('dtoken')[0].split('_', 1)[0] if (x != -1) else x), vec)))\n",
    "        vec_vocab2.append(tmp)\n",
    "    for vec in vec4:\n",
    "        tmp = (list(map((lambda x: x.get_attribute_list('dtoken')[0].split('_', 1)[0] if (x != -1) else x), vec)))\n",
    "        vec_vocab4.append(tmp)\n",
    "    return vec_vocab2, vec_vocab4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "5c3a629f-7241-439d-a942-1bf6f7e8d29b",
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1644256359295,
     "user": {
      "displayName": "Ori Suchy",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02082319045580391969"
     },
     "user_tz": -120
    },
    "id": "5c3a629f-7241-439d-a942-1bf6f7e8d29b"
   },
   "outputs": [],
   "source": [
    "# morphology\n",
    "def feature_vec_2(achs=achs, vec2 = vec2, vec4=vec4):\n",
    "    vec_morph2, vec_morph4 = [], []\n",
    "    for vec in vec2:\n",
    "        vec_morph2.append(list(map((lambda x: x.get_attribute_list('dtoken')[0].split('__', 1)[-1].split('_',1)[0] if (x != -1) else x), vec)))\n",
    "    for vec in vec4:\n",
    "        vec_morph4.append(list(map((lambda x: x.get_attribute_list('dtoken')[0].split('__', 1)[-1].split('_',1)[0] if (x != -1) else x), vec)))\n",
    "    return vec_morph2, vec_morph4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "0739d69c-a80e-4da3-9d1b-010056319614",
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1644256359296,
     "user": {
      "displayName": "Ori Suchy",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02082319045580391969"
     },
     "user_tz": -120
    },
    "id": "0739d69c-a80e-4da3-9d1b-010056319614"
   },
   "outputs": [],
   "source": [
    "#syntax\n",
    "def feature_vec_3(achs=achs, vec2 = vec2, vec4=vec4):\n",
    "    vec_syntax2, vec_syntax4 = [], []\n",
    "    for i, vec in enumerate(vec2):\n",
    "        functions = achs[i].find_all('syntacticinfo')\n",
    "        phrases = {}\n",
    "        for s in functions:\n",
    "            c = s.find_all('clause')\n",
    "            for p in c:\n",
    "                p2 = p.find_all('phrase')\n",
    "                for p in p2:\n",
    "                    phrases[p.get_attribute_list('id')[0]] = p.get_attribute_list('function')[0]\n",
    "        tmp = list(map((lambda x: x.find_all('m')[0].get_attribute_list('phraseid')[0] if (x != -1) else x), vec))\n",
    "        tmp = list(map((lambda x: phrases.get(x) if (x != -1) else x), tmp))\n",
    "        vec_syntax2.append(tmp)\n",
    "    for i, vec in enumerate(vec4):\n",
    "        functions = achs[i].find_all('syntacticinfo')\n",
    "        phrases = {}\n",
    "        for s in functions:\n",
    "            c = s.find_all('clause')\n",
    "            for p in c:\n",
    "                p2 = p.find_all('phrase')\n",
    "                for p in p2:\n",
    "                    phrases[p.get_attribute_list('id')[0]] = p.get_attribute_list('function')[0]\n",
    "        tmp = list(map((lambda x: x.find_all('m')[0].get_attribute_list('phraseid')[0] if (x != -1) else x), vec))\n",
    "        tmp = list(map((lambda x: phrases.get(x) if (x != -1) else x), tmp))\n",
    "        vec_syntax4.append(tmp)\n",
    "    return vec_syntax2, vec_syntax4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "e45c39d7-20f4-4119-910e-731d42c001eb",
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1644256359298,
     "user": {
      "displayName": "Ori Suchy",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02082319045580391969"
     },
     "user_tz": -120
    },
    "id": "e45c39d7-20f4-4119-910e-731d42c001eb"
   },
   "outputs": [],
   "source": [
    "#lemma\n",
    "def feature_vec_4(achs=achs, vec2 = vec2, vec4=vec4):\n",
    "    vec_lemma2, vec_lemma4 = [], []  \n",
    "    for vec in vec2:\n",
    "        vec_lemma2.append(list(map((lambda x: (re.sub(\"[/='[']\", \"\", x.get_attribute_list('lemma')[0])) if (x != -1) else x), vec)))\n",
    "    for vec in vec4:\n",
    "        vec_lemma4.append(list(map((lambda x: (re.sub(\"[/='[']\", \"\", x.get_attribute_list('lemma')[0])) if (x != -1) else x), vec)))\n",
    "    return vec_lemma2, vec_lemma4 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431267bb-fae4-4d7d-aea9-709776a07603",
   "metadata": {
    "id": "431267bb-fae4-4d7d-aea9-709776a07603"
   },
   "source": [
    "## Combinations of features:\n",
    "\n",
    "#### The combinations we wish to create are:: \n",
    "> (morpho X syntax) <br>\n",
    "(morpho X lemma) <br>\n",
    "(syntax X lemma)\n",
    "\n",
    "\n",
    "For example, we combine (morph X syntax) feature: <br>\n",
    "> **Morphologic:** (sub, disj) <br>\n",
    "**Syntactic:** (Conj, Verb) <br>\n",
    ">>**$\\rightarrow$ ((sub * Conj), (disj * Verb))**<br>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "f0ffc459-5b9d-4ea6-bf55-7d9c391cbbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tuple_features(vec_a_2=[], vec_b_2 =[], vec_a_4=[], vec_b_4 = []):\n",
    "    tup_2 = [((a,c),(b,d)) for ([a,b],[c,d]) in zip(vec_a_2, vec_b_2)]\n",
    "    tup_4 = [((a,e),(b,f),(c,g),(d,h)) for ((a,b,c,d),(e,f,g,h)) in zip(vec_a_4, vec_b_4)]\n",
    "    return tup_2, tup_4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "qEPf4EYg0wSs",
   "metadata": {
    "executionInfo": {
     "elapsed": 49,
     "status": "aborted",
     "timestamp": 1644256359710,
     "user": {
      "displayName": "Ori Suchy",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02082319045580391969"
     },
     "user_tz": -120
    },
    "id": "qEPf4EYg0wSs"
   },
   "outputs": [],
   "source": [
    "# All 12 vectors\n",
    "vec_vocab2, vec_vocab4 = feature_vec_1()\n",
    "vec_morph2, vec_morph4 = feature_vec_2()\n",
    "vec_syntax2, vec_syntax4 = feature_vec_3()\n",
    "vec_lemma2, vec_lemma4  = feature_vec_4()\n",
    "morph_and_syntax_2, morph_and_syntax_4 = tuple_features(vec_morph2, vec_syntax2, vec_morph4, vec_syntax4)\n",
    "morph_and_lemma_2, morph_and_lemma_4 = tuple_features(vec_morph2, vec_lemma2, vec_morph4, vec_lemma4)\n",
    "syntax_and_lemma_2, syntax_and_lemma_4 = tuple_features(vec_syntax2, vec_lemma2, vec_syntax4, vec_lemma4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WRd1QyK4zOLl",
   "metadata": {
    "id": "WRd1QyK4zOLl"
   },
   "source": [
    "---\n",
    "# Feature vectors to \"one-hot-vector\" representation:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a114b8-f4d8-402c-94ae-bfbe6ea32b34",
   "metadata": {
    "id": "42a114b8-f4d8-402c-94ae-bfbe6ea32b34"
   },
   "source": [
    "#### We create a set of keys matching the values of the feature for each.   \n",
    "#### This will match the requirements of the input for weka as \"One-Hot-Vector\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "318f0b2d-1b29-429b-b9f4-ae1e75b635b2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 238,
     "status": "ok",
     "timestamp": 1644256489952,
     "user": {
      "displayName": "Ori Suchy",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02082319045580391969"
     },
     "user_tz": -120
    },
    "id": "318f0b2d-1b29-429b-b9f4-ae1e75b635b2",
    "outputId": "cae84595-fa8e-4172-a0e2-1f62e1c5397e"
   },
   "outputs": [],
   "source": [
    "def get_keys(vec = []):\n",
    "    list_ = [inner for outer in vec for inner in outer]\n",
    "    list_ = sorted(set(list_), key=list_.index)\n",
    "    return list_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "b93e4834-6b66-4019-bd30-b444f318ef3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_name = 'inputs'\n",
    "vecs = [(vec_vocab2, vec_vocab4), (vec_morph2, vec_morph4), (vec_syntax2, vec_syntax4), (morph_and_syntax_2, morph_and_syntax_4), (morph_and_lemma_2, morph_and_lemma_4), (syntax_and_lemma_2, syntax_and_lemma_4)]\n",
    "tags = [get_keys(vecs[0][1]), get_keys(vecs[1][1]),get_keys(vecs[2][1]),get_keys(vecs[3][1]),get_keys(vecs[4][1]),get_keys(vecs[5][1])]\n",
    "names = ['vocab', 'morpho', 'syntax', 'morph_syn', 'morph_lemma', 'syn_lemma']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d82d07a-b37c-4237-859f-95345fb248b8",
   "metadata": {},
   "source": [
    "---\n",
    "# CSV files:\n",
    "#### Creating csv of 12 vectors as input weka requires:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_n6z6EDawKnV",
   "metadata": {
    "executionInfo": {
     "elapsed": 45,
     "status": "aborted",
     "timestamp": 1644256359706,
     "user": {
      "displayName": "Ori Suchy",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02082319045580391969"
     },
     "user_tz": -120
    },
    "id": "_n6z6EDawKnV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "4003c22f-92e0-4664-8fe4-9c54ecc9a047",
   "metadata": {
    "executionInfo": {
     "elapsed": 47,
     "status": "aborted",
     "timestamp": 1644256359708,
     "user": {
      "displayName": "Ori Suchy",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02082319045580391969"
     },
     "user_tz": -120
    },
    "id": "4003c22f-92e0-4664-8fe4-9c54ecc9a047"
   },
   "outputs": [],
   "source": [
    "def index_vector(tags=[], vec2=[], vec4 =[], name = \"\", dir_name_inputs = 'inputs', ach_tags_ = ach_tags):\n",
    "    if not os.path.exists(dir_name_inputs):\n",
    "        os.mkdir(dir_name_inputs)\n",
    "    if not os.path.exists('temps'):\n",
    "        os.mkdir('temps')\n",
    "    input_vector_4 = [(tags.index(x), tags.index(y),tags.index(z),tags.index(w)) for (x,y,z,w) in vec4] \n",
    "    df4 = pd.DataFrame(input_vector_4)\n",
    "    df4.to_csv(os.path.join(folder_path,'temps', name+\"4\"), index=False, header=['w-2', 'w-1', 'w+1', 'w+2'])\n",
    "    df4 = pd.read_csv(os.path.join(folder_path, 'temps', name+\"4\"))\n",
    "    df = pd.concat([df4['w-2'], df4['w-1'], df4['w+1'], df4['w+2'], ach_tags_], axis=1)\n",
    "    df.to_csv(os.path.join(folder_path, dir_name_inputs, name+\"4.csv\"), index=False)\n",
    "    input_vector_2 = [(tags.index(x), tags.index(y)) for (x,y) in vec2]\n",
    "    df2 = pd.DataFrame(input_vector_2)\n",
    "    df2.to_csv(os.path.join(folder_path, 'temps', name+\"2\"), index=False, header=['w-1', 'w+1'])\n",
    "    df2 = pd.read_csv(os.path.join(folder_path, 'temps', name+\"2\"))\n",
    "    df = pd.concat([df2['w-1'], df2['w+1'], ach_tags_], axis=1)\n",
    "    df.to_csv(os.path.join(folder_path, dir_name_inputs, name+\"2.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "41018686-5487-4291-be03-1955ddb34e35",
   "metadata": {
    "executionInfo": {
     "elapsed": 53,
     "status": "aborted",
     "timestamp": 1644256359715,
     "user": {
      "displayName": "Ori Suchy",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02082319045580391969"
     },
     "user_tz": -120
    },
    "id": "41018686-5487-4291-be03-1955ddb34e35"
   },
   "outputs": [],
   "source": [
    "for i in range(6):\n",
    "    index_vector(tags=tags[i], vec2=vecs[i][0], vec4 =vecs[i][1], name = names[i], dir_name_inputs=dir_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4fab19-83e8-4902-b975-423d5ef04559",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac8ed45-223c-4adf-880a-75940a0534b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c188562a-a060-4d40-b3f8-5ff64d4ab456",
   "metadata": {},
   "source": [
    "---\n",
    "# Testing modifications on the input\n",
    "\n",
    ">size - try half the size<br>\n",
    "regularized - eqaul range of tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c4ae0e-3b39-469b-ae18-ff4e6f077238",
   "metadata": {},
   "source": [
    "### Regularized subset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "3340f25f-31f0-4cc0-91ce-33612f7f3b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_indices():\n",
    "    words = ['אכן','רק','אבל','זה עתה']\n",
    "    rak, aval, ata, achen = 0,0,0,0\n",
    "    regs = []\n",
    "    for i, tag in enumerate(ach_tags):\n",
    "        if tag == words[0] and achen < 24:\n",
    "            achen += 1\n",
    "            regs.append(i)\n",
    "        elif tag == words[1] and rak < 24:\n",
    "            regs.append(i)\n",
    "            rak += 1\n",
    "        elif tag == words[2] and aval < 24:\n",
    "            regs.append(i)\n",
    "            aval +=1\n",
    "        elif tag == words[3] and ata < 24:\n",
    "            regs.append(i)\n",
    "    return regs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "52dd750d-c46b-4100-bb50-3fcb5274ee56",
   "metadata": {},
   "outputs": [],
   "source": [
    "achs_regularized_indiced = get_indices()\n",
    "achs_reg_xml = [achs[i] for i in achs_regularized_indiced]\n",
    "achs_reg_csv = pd.DataFrame([ach_tags[i] for i in achs_regularized_indiced])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "94795b09-c135-442f-9ae5-1177c9928609",
   "metadata": {
    "executionInfo": {
     "elapsed": 49,
     "status": "aborted",
     "timestamp": 1644256359710,
     "user": {
      "displayName": "Ori Suchy",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02082319045580391969"
     },
     "user_tz": -120
    },
    "id": "qEPf4EYg0wSs"
   },
   "outputs": [],
   "source": [
    "# All 12 vectors of regularized\n",
    "vec2_reg, vec4_reg = w_vec(achs=achs_reg_xml) \n",
    "vec_vocab2_reg, vec_vocab4_reg = feature_vec_1(achs=achs_reg_xml, vec2=vec2_reg, vec4=vec4_reg)\n",
    "vec_morph2_reg, vec_morph4_reg = feature_vec_2(achs=achs_reg_xml, vec2=vec2_reg, vec4=vec4_reg)\n",
    "vec_syntax2_reg, vec_syntax4_reg = feature_vec_3(achs=achs_reg_xml, vec2=vec2_reg, vec4=vec4_reg)\n",
    "vec_lemma2_reg, vec_lemma4_reg  = feature_vec_4(achs=achs_reg_xml, vec2=vec2_reg, vec4=vec4_reg)\n",
    "morph_and_syntax_2_reg, morph_and_syntax_4_reg = tuple_features(vec_morph2_reg, vec_syntax2_reg, vec_morph4_reg, vec_syntax4_reg)\n",
    "morph_and_lemma_2_reg, morph_and_lemma_4_reg = tuple_features(vec_morph2_reg, vec_lemma2_reg, vec_morph4_reg, vec_lemma4_reg)\n",
    "syntax_and_lemma_2_reg, syntax_and_lemma_4_reg = tuple_features(vec_syntax2_reg, vec_lemma2_reg, vec_syntax4_reg, vec_lemma4_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "50a41d37-4dd8-4bf4-aa5e-c5ee9789ef59",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_name_reg = 'inputs_reg'\n",
    "vecs_reg = [(vec_vocab2_reg, vec_vocab4_reg), (vec_morph2_reg, vec_morph4_reg), (vec_syntax2_reg, vec_syntax4_reg), (morph_and_syntax_2_reg, morph_and_syntax_4_reg), (morph_and_lemma_2_reg, morph_and_lemma_4_reg), (syntax_and_lemma_2_reg, syntax_and_lemma_4_reg)]\n",
    "tags_reg = [get_keys(vecs_reg[0][1]), get_keys(vecs_reg[1][1]),get_keys(vecs_reg[2][1]),get_keys(vecs_reg[3][1]),get_keys(vecs_reg[4][1]),get_keys(vecs_reg[5][1])]\n",
    "names_reg = ['vocab', 'morpho', 'syntax', 'morph_syn', 'morph_lemma', 'syn_lemma']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "c4bafb1b-1d02-4c18-ad6e-07fe71df5d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(6):\n",
    "    index_vector(tags=tags_reg[i], vec2=vecs_reg[i][0], vec4 =vecs_reg[i][1], name = names_reg[i], dir_name_inputs=dir_name_reg, ach_tags_=achs_reg_csv[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae334ba-3a8d-43a7-93e0-9060d7f6fa7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "29d00b99-112c-491a-8e55-87a1bdc52d62",
   "metadata": {},
   "source": [
    "### Random subset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "6758566b-57a4-4cfc-b23c-da0765ad7d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "achs_rand_indices = sorted(random.sample(range(161), 80))\n",
    "achs_rand_xml = [achs[i] for i in achs_rand_indices]\n",
    "achs_rand_csv = pd.DataFrame([ach_tags[i] for i in achs_rand_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "40429c53-2e46-4128-9381-e4c0f799b8ba",
   "metadata": {
    "executionInfo": {
     "elapsed": 49,
     "status": "aborted",
     "timestamp": 1644256359710,
     "user": {
      "displayName": "Ori Suchy",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02082319045580391969"
     },
     "user_tz": -120
    },
    "id": "qEPf4EYg0wSs"
   },
   "outputs": [],
   "source": [
    "# All 12 vectors of regularized\n",
    "vec2_rand, vec4_rand = w_vec(achs=achs_rand_xml) \n",
    "vec_vocab2_rand, vec_vocab4_rand = feature_vec_1(achs=achs_rand_xml, vec2=vec2_rand, vec4=vec4_rand)\n",
    "vec_morph2_rand, vec_morph4_rand = feature_vec_2(achs=achs_rand_xml, vec2=vec2_rand, vec4=vec4_rand)\n",
    "vec_syntax2_rand, vec_syntax4_rand = feature_vec_3(achs=achs_rand_xml, vec2=vec2_rand, vec4=vec4_rand)\n",
    "vec_lemma2_rand, vec_lemma4_rand  = feature_vec_4(achs=achs_rand_xml, vec2=vec2_rand, vec4=vec4_rand)\n",
    "morph_and_syntax_2_rand, morph_and_syntax_4_rand = tuple_features(vec_morph2_rand, vec_syntax2_rand, vec_morph4_rand, vec_syntax4_rand)\n",
    "morph_and_lemma_2_rand, morph_and_lemma_4_rand = tuple_features(vec_morph2_rand, vec_lemma2_rand, vec_morph4_rand, vec_lemma4_rand)\n",
    "syntax_and_lemma_2_rand, syntax_and_lemma_4_rand = tuple_features(vec_syntax2_rand, vec_lemma2_rand, vec_syntax4_rand, vec_lemma4_rand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "c448e42a-0d1a-40aa-ad56-ed1ae5236f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_name_rand = 'inputs_rand'\n",
    "vecs_rand = [(vec_vocab2_rand, vec_vocab4_rand), (vec_morph2_rand, vec_morph4_rand), (vec_syntax2_rand, vec_syntax4_rand), (morph_and_syntax_2_rand, morph_and_syntax_4_rand), (morph_and_lemma_2_rand, morph_and_lemma_4_rand), (syntax_and_lemma_2_rand, syntax_and_lemma_4_rand)]\n",
    "tags_rand = [get_keys(vecs_rand[0][1]), get_keys(vecs_rand[1][1]),get_keys(vecs_rand[2][1]),get_keys(vecs_rand[3][1]),get_keys(vecs_rand[4][1]),get_keys(vecs_rand[5][1])]\n",
    "names_rand = ['vocab', 'morpho', 'syntax', 'morph_syn', 'morph_lemma', 'syn_lemma']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "205452fe-5124-4992-8139-ee97fcb3829b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(6):\n",
    "    index_vector(tags=tags_rand[i], vec2=vecs_rand[i][0], vec4 =vecs_rand[i][1], name = names_rand[i], dir_name_inputs=dir_name_rand, ach_tags_=achs_rand_csv[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be04e5b4-47af-4fe7-8f06-091c58777cb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "de701579-a33c-4455-8706-181ba3ed4877"
   ],
   "name": "vectors.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
